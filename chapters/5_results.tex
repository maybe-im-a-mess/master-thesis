% !TeX encoding = UTF-8
\chapter{Experiments and results}

This chapter will present the results of the experiments conducted to evaluate the performance of the proposed chatbot system. The results will be organized according to the evaluation framework outlined in Chapter 4, covering information extraction benchmarking, chatbot communication quality assessment, and dialogue management efficacy. Each section will include a detailed analysis of the results, highlighting key findings and their implications for the research objectives.

\section{Information Extraction Benchmarking}

TODO:
Comparing the performance of GPT-5-mini, qwen3-32b, ministral-3-14b, and gpt-oss-20b across the four prompting strategies (Naive, CoT, Atomic Fact, UIE).

\section{Chatbot Communication Quality Assessment}

TODO:
Analyzing the results of the sender-side and recipient-side evaluations, including topic coverage, citation accuracy

\subsection{Sender-Side Metrics Analysis}

TODO:
Analyzing the results of topic coverage, citation accuracy, and response conciseness from the Doctor Agent's perspective.

\subsection{Recipient-Side Metrics Analysis}

TODO:
Analyzing the results of patient comprehension testing and other relevant metrics from the Patient Agent's perspective.

\section{Dialogue Management Efficacy}

TODO:
Comparing the naive dialogue manager against the supervised manager in successfully gathering mandatory information.