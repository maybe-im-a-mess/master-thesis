@article{dagdelenStructuredInformationExtraction2024,
  title = {Structured Information Extraction from Scientific Text with Large Language Models},
  author = {Dagdelen, John and Dunn, Alexander and Lee, Sanghoon and Walker, Nicholas and Rosen, Andrew S. and Ceder, Gerbrand and Persson, Kristin A. and Jain, Anubhav},
  year = 2024,
  month = feb,
  journal = {Nature Communications},
  volume = {15},
  number = {1},
  pages = {1418},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-45563-x},
  urldate = {2025-12-14},
  abstract = {Extracting structured knowledge from scientific text remains a challenging task for machine learning models. Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pretrained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Databases,Materials science,Scientific data,Theory and computation},
  file = {/Users/olha/Zotero/storage/TTTK8BRU/Dagdelen et al. - 2024 - Structured information extraction from scientific text with large language models.pdf}
}

@misc{edgeLocalGlobalGraph2025,
  title = {From {{Local}} to {{Global}}: {{A Graph RAG Approach}} to {{Query-Focused Summarization}}},
  shorttitle = {From {{Local}} to {{Global}}},
  author = {Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Metropolitansky, Dasha and Ness, Robert Osazuwa and Larson, Jonathan},
  year = 2025,
  month = feb,
  number = {arXiv:2404.16130},
  eprint = {2404.16130},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.16130},
  urldate = {2025-12-02},
  abstract = {The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as "What are the main themes in the dataset?", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose GraphRAG, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text. Our approach uses an LLM to build a graph index in two stages: first, to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/Users/olha/Zotero/storage/RC8MXYWP/Edge et al. - 2025 - From Local to Global A Graph RAG Approach to Query-Focused Summarization.pdf;/Users/olha/Zotero/storage/J7XQMJJ7/2404.html}
}

@misc{geroSelfVerificationImprovesFewShot2023,
  title = {Self-{{Verification Improves Few-Shot Clinical Information Extraction}}},
  author = {Gero, Zelalem and Singh, Chandan and Cheng, Hao and Naumann, Tristan and Galley, Michel and Gao, Jianfeng and Poon, Hoifung},
  year = 2023,
  month = may,
  number = {arXiv:2306.00024},
  eprint = {2306.00024},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.00024},
  urldate = {2025-12-02},
  abstract = {Extracting patient information from unstructured text is a critical task in health decision-support and clinical research. Large language models (LLMs) have shown the potential to accelerate clinical curation via few-shot in-context learning, in contrast to supervised learning which requires much more costly human annotations. However, despite drastic advances in modern LLMs such as GPT-4, they still struggle with issues regarding accuracy and interpretability, especially in mission-critical domains such as health. Here, we explore a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs. This is made possible by the asymmetry between verification and generation, where the latter is often much easier than the former. Experimental results show that our method consistently improves accuracy for various LLMs in standard clinical information extraction tasks. Additionally, self-verification yields interpretations in the form of a short text span corresponding to each output, which makes it very efficient for human experts to audit the results, paving the way towards trustworthy extraction of clinical information in resource-constrained scenarios. To facilitate future research in this direction, we release our code and prompts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/olha/Zotero/storage/RDXW5QVT/Gero et al. - 2023 - Self-Verification Improves Few-Shot Clinical Information Extraction.pdf;/Users/olha/Zotero/storage/97DDX7HW/2306.html}
}

@misc{liangSchemaParameterizedTools2025,
  title = {Schema as {{Parameterized Tools}} for {{Universal Information Extraction}}},
  author = {Liang, Sheng and Zhang, Yongyue and Wu, Yaxiong and Tang, Ruiming and Liu, Yong},
  year = 2025,
  month = jun,
  number = {arXiv:2506.01276},
  eprint = {2506.01276},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.01276},
  urldate = {2025-12-15},
  abstract = {Universal information extraction (UIE) primarily employs an extractive generation approach with large language models (LLMs), typically outputting structured information based on predefined schemas such as JSON or tables. UIE suffers from a lack of adaptability when selecting between predefined schemas and on-the-fly schema generation within the in-context learning paradigm, especially when there are numerous schemas to choose from. In this paper, we propose a unified adaptive text-to-structure generation framework, called Schema as Parameterized Tools (SPT), which reimagines the tool-calling capability of LLMs by treating predefined schemas as parameterized tools for tool selection and parameter filling. Specifically, our SPT method can be applied to unify closed, open, and on-demand IE tasks by adopting Schema Retrieval by fetching the relevant schemas from a predefined pool, Schema Filling by extracting information and filling slots as with tool parameters, or Schema Generation by synthesizing new schemas with uncovered cases. Experiments show that the SPT method can handle four distinct IE tasks adaptively, delivering robust schema retrieval and selection performance. SPT also achieves comparable extraction performance to LoRA baselines and current leading UIE systems with significantly fewer trainable parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/olha/Zotero/storage/PF8HQA9A/Liang et al. - 2025 - Schema as Parameterized Tools for Universal Information Extraction.pdf;/Users/olha/Zotero/storage/6MSEA7VV/2506.html}
}

@misc{luoOneKEDockerizedSchemaGuided2025,
  title = {{{OneKE}}: {{A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System}}},
  shorttitle = {{{OneKE}}},
  author = {Luo, Yujie and Ru, Xiangyuan and Liu, Kangwei and Yuan, Lin and Sun, Mengshu and Zhang, Ningyu and Liang, Lei and Zhang, Zhiqiang and Zhou, Jun and Wei, Lanning and Zheng, Da and Wang, Haofen and Chen, Huajun},
  year = 2025,
  month = feb,
  number = {arXiv:2412.20005},
  eprint = {2412.20005},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.20005},
  urldate = {2025-12-15},
  abstract = {We introduce OneKE, a dockerized schema-guided knowledge extraction system, which can extract knowledge from the Web and raw PDF Books, and support various domains (science, news, etc.). Specifically, we design OneKE with multiple agents and a configure knowledge base. Different agents perform their respective roles, enabling support for various extraction scenarios. The configure knowledge base facilitates schema configuration, error case debugging and correction, further improving the performance. Empirical evaluations on benchmark datasets demonstrate OneKE's efficacy, while case studies further elucidate its adaptability to diverse tasks across multiple domains, highlighting its potential for broad applications. We have open-sourced the Code at https://github.com/zjunlp/OneKE and released a Video at http://oneke.openkg.cn/demo.mp4.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Databases,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/olha/Zotero/storage/Z6VW7KRX/Luo et al. - 2025 - OneKE A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System.pdf;/Users/olha/Zotero/storage/Q3KWUFBC/2412.html}
}

@inproceedings{luUnifiedStructureGeneration2022,
  title = {Unified {{Structure Generation}} for {{Universal Information Extraction}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lu, Yaojie and Liu, Qing and Dai, Dai and Xiao, Xinyan and Lin, Hongyu and Han, Xianpei and Sun, Le and Wu, Hua},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  year = 2022,
  month = may,
  pages = {5755--5772},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.395},
  urldate = {2025-12-02},
  abstract = {Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism -- structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.},
  file = {/Users/olha/Zotero/storage/T9ZH37NT/Lu et al. - 2022 - Unified Structure Generation for Universal Information Extraction.pdf}
}

@inproceedings{maChainThoughtExplicit2023,
  title = {Chain of {{Thought}} with {{Explicit Evidence Reasoning}} for {{Few-shot Relation Extraction}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Ma, Xilai and Li, Jing and Zhang, Min},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = 2023,
  month = dec,
  pages = {2334--2352},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.153},
  urldate = {2025-12-02},
  abstract = {Few-shot relation extraction involves identifying the type of relationship between two specific entities within a text, using a limited number of annotated samples. A variety of solutions to this problem have emerged by applying meta-learning and neural graph techniques which typically necessitate a training process for adaptation. Recently, the strategy of in-context learning has been demonstrating notable results without the need of training. Few studies have already utilized in-context learning for zero-shot information extraction. Unfortunately, the evidence for inference is either not considered or implicitly modeled during the construction of chain-of-thought prompts. In this paper, we propose a novel approach for few-shot relation extraction using large language models, named CoT-ER, chain-of-thought with explicit evidence reasoning. In particular, CoT-ER first induces large language models to generate evidences using task-specific and concept-level knowledge. Then these evidences are explicitly incorporated into chain-of-thought prompting for relation extraction. Experimental results demonstrate that our CoT-ER approach (with 0\% training data) achieves competitive performance compared to the fully-supervised (with 100\% training data) state-of-the-art approach on the FewRel1.0 and FewRel2.0 datasets.},
  file = {/Users/olha/Zotero/storage/Y6AR3WD4/Ma et al. - 2023 - Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction.pdf}
}

@inproceedings{minFActScoreFinegrainedAtomic2023,
  title = {{{FActScore}}: {{Fine-grained Atomic Evaluation}} of {{Factual Precision}} in {{Long Form Text Generation}}},
  shorttitle = {{{FActScore}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = 2023,
  month = dec,
  pages = {12076--12100},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.741},
  urldate = {2025-12-02},
  abstract = {Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs---InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI---and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58\%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2\% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost \$26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via `pip install factscore`.},
  file = {/Users/olha/Zotero/storage/QCGK2DAD/Min et al. - 2023 - FActScore Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation.pdf}
}

@misc{sainzGoLLIEAnnotationGuidelines2024,
  title = {{{GoLLIE}}: {{Annotation Guidelines}} Improve {{Zero-Shot Information-Extraction}}},
  shorttitle = {{{GoLLIE}}},
  author = {Sainz, Oscar and {Garc{\'i}a-Ferrero}, Iker and Agerri, Rodrigo and de Lacalle, Oier Lopez and Rigau, German and Agirre, Eneko},
  year = 2024,
  month = mar,
  number = {arXiv:2310.03668},
  eprint = {2310.03668},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.03668},
  urldate = {2025-12-02},
  abstract = {Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation guidelines which describe the task and give examples to humans. Previous attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out-of-the-box. In this paper we propose GoLLIE (Guidelinefollowing Large Language Model for IE), a model able to improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation guidelines. Comprehensive evaluation empirically demonstrates that GoLLIE is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction. The ablation study shows that detailed guidelines is key for good results. Code, data and models are publicly available: https://github.com/hitz-zentroa/GoLLIE.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/olha/Zotero/storage/IZSYHCZF/Sainz et al. - 2024 - GoLLIE Annotation Guidelines improve Zero-Shot Information-Extraction.pdf}
}

@misc{schmidtDissectingAtomicFacts2025,
  title = {Dissecting {{Atomic Facts}}: {{Visual Analytics}} for {{Improving Fact Annotations}} in {{Language Model Evaluation}}},
  shorttitle = {Dissecting {{Atomic Facts}}},
  author = {Schmidt, Manuel and Keim, Daniel A. and Dennig, Frederik L.},
  year = 2025,
  month = sep,
  number = {arXiv:2509.01460},
  eprint = {2509.01460},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.01460},
  urldate = {2025-12-02},
  abstract = {Factuality evaluation of large language model (LLM) outputs requires decomposing text into discrete "atomic" facts. However, existing definitions of atomicity are underspecified, with empirical results showing high disagreement among annotators, both human and model-based, due to unresolved ambiguity in fact decomposition. We present a visual analytics concept to expose and analyze annotation inconsistencies in fact extraction. By visualizing semantic alignment, granularity and referential dependencies, our approach aims to enable systematic inspection of extracted facts and facilitate convergence through guided revision loops, establishing a more stable foundation for factuality evaluation benchmarks and improving LLM evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/Users/olha/Zotero/storage/ZJEJKKF2/Schmidt et al. - 2025 - Dissecting Atomic Facts Visual Analytics for Improving Fact Annotations in Language Model Evaluatio.pdf;/Users/olha/Zotero/storage/PDTA9P4I/2509.html}
}

@misc{wangInstructUIEMultitaskInstruction2023,
  title = {{{InstructUIE}}: {{Multi-task Instruction Tuning}} for {{Unified Information Extraction}}},
  shorttitle = {{{InstructUIE}}},
  author = {Wang, Xiao and Zhou, Weikang and Zu, Can and Xia, Han and Chen, Tianze and Zhang, Yuansen and Zheng, Rui and Ye, Junjie and Zhang, Qi and Gui, Tao and Kang, Jihua and Yang, Jingsheng and Li, Siyuan and Du, Chunsai},
  year = 2023,
  month = apr,
  number = {arXiv:2304.08085},
  eprint = {2304.08085},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.08085},
  urldate = {2025-12-02},
  abstract = {Large language models have unlocked strong multi-task capabilities from reading instructive prompts. However, recent studies have shown that existing large models still have difficulty with information extraction tasks. For example, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset, which is significantly lower than the state-of-the-art performance. In this paper, we propose InstructUIE, a unified information extraction framework based on instruction tuning, which can uniformly model various information extraction tasks and capture the inter-task dependency. To validate the proposed method, we introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions. Experimental results demonstrate that our method achieves comparable performance to Bert in supervised settings and significantly outperforms the state-of-the-art and gpt3.5 in zero-shot settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/olha/Zotero/storage/CHKU8BNR/Wang et al. - 2023 - InstructUIE Multi-task Instruction Tuning for Unified Information Extraction.pdf;/Users/olha/Zotero/storage/CDGDITF5/2304.html}
}

@misc{zhengFactFragmentsDeconstructing2025,
  title = {Fact in {{Fragments}}: {{Deconstructing Complex Claims}} via {{LLM-based Atomic Fact Extraction}} and {{Verification}}},
  shorttitle = {Fact in {{Fragments}}},
  author = {Zheng, Liwen and Li, Chaozhuo and Liu, Zheng and Huang, Feiran and Jia, Haoran and Ye, Zaisheng and Zhang, Xi},
  year = 2025,
  month = jun,
  number = {arXiv:2506.07446},
  eprint = {2506.07446},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.07446},
  urldate = {2025-12-02},
  abstract = {Fact verification plays a vital role in combating misinformation by assessing the veracity of claims through evidence retrieval and reasoning. However, traditional methods struggle with complex claims requiring multi-hop reasoning over fragmented evidence, as they often rely on static decomposition strategies and surface-level semantic retrieval, which fail to capture the nuanced structure and intent of the claim. This results in accumulated reasoning errors, noisy evidence contamination, and limited adaptability to diverse claims, ultimately undermining verification accuracy in complex scenarios. To address this, we propose Atomic Fact Extraction and Verification (AFEV), a novel framework that iteratively decomposes complex claims into atomic facts, enabling fine-grained retrieval and adaptive reasoning. AFEV dynamically refines claim understanding and reduces error propagation through iterative fact extraction, reranks evidence to filter noise, and leverages context-specific demonstrations to guide the reasoning process. Extensive experiments on five benchmark datasets demonstrate that AFEV achieves state-of-the-art performance in both accuracy and interpretability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/olha/Zotero/storage/TMIQ5CKM/Zheng et al. - 2025 - Fact in Fragments Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verificati.pdf;/Users/olha/Zotero/storage/QSQIQX2L/2506.html}
}
